{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let ${(X_1, Y_1),(X_2, Y_2), . . . ,(X_m, Y_m)}$ denote a data set, where $X_i$ represents a vector of k (binary) feature values,\n",
    "and $Y_i$ is a corresponding binary class or label that we will need to learn to be able to predict from the X-values.\n",
    "We generate data via the following scheme, defining a distribution for our data set: Let $X = (X_1, X_2, X_3, . . . , X_k)$\n",
    "be a vector of binary values, satisfying the following\n",
    "- $X_1 = 1$ with probability 1/2, $X_1 = 0$ with probability 1/2\n",
    "- For i = 2, . . . , k, $X_i = X_{i−1}$ with probability 3/4, and $X_i = 1 − X_{i−1}$ with probability 1/4.\n",
    "In this way, the first feature value is uniformly random, but every successive feature is strongly correlated with the\n",
    "value of the feature before it. We can then define Y to be a function of X as\n",
    "$$Y = X_1 if w_2X_2 + w_3X_3 + . . . + w_kX_k ≥ 1/2$$\n",
    "$$Y = 1 − X_1 else$$\n",
    "\n",
    "In other words, if the ‘weighted average’ of $X_2, . . . X_k$ tilts high, Y will agree with $X_1$; if the weighted average of\n",
    "$X_2, . . . , X_k$ tilts low, Y will disagree with $X_1$. Take the weights to be defined by $w_i = \\frac{0.9^i}{0.9^2 + 0.9^3 + ... + 0.9^k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. For a given value of k, m, (number of features, number of data points), write a function to generate a training data set based on the above scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X (feature) vectors for the data\n",
    "def create_data(k, m):\n",
    "    X = [[0]*k for i in range(m)]\n",
    "    for i in range(m):\n",
    "        X[i][0] = int(np.random.choice(2, size=1))\n",
    "        for j in range(1, k):\n",
    "            temp = np.random.choice(2, 1, p=[0.25,0.75])\n",
    "            if temp == 1:\n",
    "                X[i][j] = X[i][j-1]\n",
    "            else:\n",
    "                X[i][j] = 1 - X[i][j-1]\n",
    "    return X\n",
    "\n",
    "# Creating weights for the data\n",
    "def create_weights(k):\n",
    "    div = 0\n",
    "    weight = [0]*(k+1)\n",
    "    for i in range(2, k+1):\n",
    "        div += 0.9**i\n",
    "    for i in range(1, k+1):\n",
    "        weight[i] = (0.9**i)/div\n",
    "        \n",
    "    return weight[1:]\n",
    " \n",
    "# Creating target column for the data\n",
    "def create_y(X, w, k, m):\n",
    "    y = []\n",
    "    for i in range(m):\n",
    "        val = np.dot(X[i][1:], w[1:].T)\n",
    "#         print(val)\n",
    "        if val < 0.5:\n",
    "            y.append(1 - X[i][0])\n",
    "        else:\n",
    "            y.append(X[i][0])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the sub data points into a dataframe\n",
    "def create_dataset(k, m):\n",
    "    X = np.asarray(create_data(k, m))\n",
    "    w = np.asarray(create_weights(k))\n",
    "    y = np.asarray(create_y(X, w, k, m)).reshape((m,1))\n",
    "\n",
    "#     For clarity in dataframe, I have added these column names. Due to this, the max number of columns can only be 26.\n",
    "#     Since, this assignment does not ask for that many columns, there should be no problem with this implementation\n",
    "    cols = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "    # Training data is an appended version of X and y arrays\n",
    "    data = pd.DataFrame(np.append(X, y, axis=1), columns=cols[:k+1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  b  c  d  e\n",
       "0   0  0  0  0  1\n",
       "1   0  0  1  1  0\n",
       "2   1  1  1  1  1\n",
       "3   1  1  1  1  1\n",
       "4   0  0  0  1  1\n",
       "5   0  0  1  1  0\n",
       "6   0  0  0  0  1\n",
       "7   1  1  1  1  1\n",
       "8   0  1  1  1  0\n",
       "9   1  1  1  1  1\n",
       "10  1  1  1  1  1\n",
       "11  0  0  0  1  1\n",
       "12  1  1  0  1  1\n",
       "13  0  0  0  1  1\n",
       "14  0  0  0  1  1\n",
       "15  0  0  0  0  1\n",
       "16  1  1  0  1  1\n",
       "17  1  0  0  0  0\n",
       "18  1  1  1  1  1\n",
       "19  0  0  0  0  1\n",
       "20  0  0  1  1  0\n",
       "21  1  1  1  0  1\n",
       "22  1  1  1  0  1\n",
       "23  0  0  0  0  1\n",
       "24  1  1  1  1  1\n",
       "25  0  1  1  1  0\n",
       "26  0  0  1  1  0\n",
       "27  1  0  1  0  0\n",
       "28  0  0  0  0  1\n",
       "29  1  0  0  0  0"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global variables, k - number of features, m - sample size, epsilon - a very small value (e-16) used to avoid divide by zero errors\n",
    "k, m = 4, 30\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "train_data = create_dataset(k, m)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Given a data set, write a function to fit a decision tree to that data based on splitting the variables by maximizing the information gain. Additionally, return the training error of this tree on the data set, $err_{train}(\\hat{f})$. It may be useful to have a function that takes a data set and a variable, and returns the data set partitioned based on the values of that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Decision Tree\n",
    "class DecisionTree(): \n",
    "    '''\n",
    "    Entropy function calculates the entropy of unique values in the target data i.e. entropy for 0 and 1\n",
    "    Input - dataset\n",
    "    Return - Entropy value for target\n",
    "    '''\n",
    "    def entropy(self, data):\n",
    "#         Fetching the last column key (target column)\n",
    "        target = data.keys()[-1]\n",
    "        entropy_y = 0\n",
    "#         Listing the unique values of target variable, here it is 0 and 1\n",
    "        target_vals = data[target].unique()\n",
    "        \n",
    "        for val in target_vals:\n",
    "            p = data[target].value_counts()[val]/len(data[target])\n",
    "            entropy_y += -p*np.log2(p)\n",
    "        return entropy_y\n",
    "    \n",
    "    '''\n",
    "    Calculates the conditional entropy of the target variable w.r.t to the features i.e. H(Y|X)\n",
    "    Input - dataset, feature\n",
    "    Return - Conditional entropy    \n",
    "    '''\n",
    "    def conditional_entropy(self, data, feature):\n",
    "#         Fetching the last column key (target column)\n",
    "        target = data.keys()[-1]\n",
    "#         Listing the unique values of target variable, here it is 0 and 1\n",
    "        target_vals = data[target].unique()\n",
    "#         Listing the unique values of current feature variable, here it is 0 and 1\n",
    "        feature_vals = data[feature].unique()\n",
    "        cond_entropy_y = 0\n",
    "        \n",
    "#         Going over the unique values of current feature, and calculation the cross-entropy\n",
    "        for fval in feature_vals:\n",
    "            entropy = 0\n",
    "            for tval in target_vals:\n",
    "#                 num calculates the number of data points that satisfy the feature and target values. Example - data points which have y as 0 and x as 0\n",
    "                num = len(data[feature][data[feature] == fval][data[target] == tval])\n",
    "#                 denom calculates the total number of data points satisfying feature = 0 or 1 (depends on fval)\n",
    "                denom = len(data[feature][data[feature] == fval])\n",
    "                e = num/(denom + epsilon)\n",
    "                entropy += -(e)*np.log2(e + epsilon)\n",
    "            cond_entropy_y += -(denom/len(data))*entropy\n",
    "            \n",
    "        return abs(cond_entropy_y)\n",
    "    \n",
    "    '''\n",
    "    Calculates the impurity of a feature \n",
    "    '''\n",
    "    def gini_index_calculation(self, data, feature):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Calculates information gain value\n",
    "    Input - dataset\n",
    "    Return - max value of information gain feature\n",
    "    '''\n",
    "    def information_gain_split(self, data):\n",
    "        IG = []\n",
    "#         For every feature except the last column(y) in the dataset\n",
    "        for key in data.keys()[:-1]:\n",
    "            IG.append(self.entropy(data) - self.conditional_entropy(data, key))\n",
    "        \n",
    "        return data.keys()[:-1][np.argmax(IG)]\n",
    "    \n",
    "    '''\n",
    "    Trims down the dataset as per the information gain node. Helps in building tree\n",
    "    Input - dataset, node(which is the best split feature), val is either 0 or 1\n",
    "    Return - trimmed dataset\n",
    "    '''\n",
    "    def get_subset(self, data, node, value):\n",
    "        return data[data[node] == value].reset_index(drop=True)\n",
    "    \n",
    "    '''\n",
    "    Builds the decision tree based on functions written above. It is a recursive function till leaf nodes found\n",
    "    Input - dataset\n",
    "    Return - the built decision tree, in a dictionary like format\n",
    "    '''\n",
    "    def build_tree(self, data, tree=None):\n",
    "        target = data.keys()[-1]\n",
    "        best_split = self.information_gain_split(data)\n",
    "        feature_vals = data[best_split].unique()\n",
    "        \n",
    "        if tree is None:\n",
    "            tree = {}\n",
    "            tree[best_split] = {}\n",
    "        \n",
    "            \n",
    "        for val in feature_vals:\n",
    "            subset = self.get_subset(data, best_split, val)\n",
    "            target_val, target_counts = np.unique(subset[subset.keys()[-1]], return_counts=True)\n",
    "#             print(target_val, target_counts)\n",
    "            if len(target_counts) == 1:\n",
    "                tree[best_split][val] = target_val[0]\n",
    "            else:\n",
    "                tree[best_split][val] = self.build_tree(subset)        \n",
    "\n",
    "        return tree\n",
    "    \n",
    "    '''\n",
    "    Predicts the target value based on a data vector\n",
    "    Input - a single row of dataset or a single X vector, decision tree\n",
    "    Return - predicted value\n",
    "    '''\n",
    "    def predict(self, instance_data, tree):\n",
    "        for node in tree.keys():\n",
    "            value = instance_data[node]\n",
    "            tree = tree[node][value]\n",
    "            prediction = 0\n",
    "            \n",
    "            if type(tree) is dict:\n",
    "                prediction = self.predict(instance_data, tree)\n",
    "            else:\n",
    "                prediction = tree\n",
    "                break\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    '''\n",
    "    Predicts the target value and then calculates error based on the predictions\n",
    "    Input - dataset, decision tree built\n",
    "    Return - error\n",
    "    '''\n",
    "    def fit(self, data, tree):\n",
    "        error = 0\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data.iloc[i], tree)\n",
    "            if prediction != data.iloc[i][-1]:\n",
    "                error += 1\n",
    "        return error/len(data)    \n",
    "    \n",
    "    '''\n",
    "    Generates multiple datasets and finds error on those datasets\n",
    "    Input - Built decision tree, feature values, sample size of dataset\n",
    "    Return - typical error\n",
    "    '''\n",
    "    def generate_data_and_typical_error(self, tree, k, m):\n",
    "        typical_error = 0\n",
    "        for i in tqdm(range(500)):\n",
    "            data = create_dataset(k, m)\n",
    "            typical_error += self.fit(data, tree)\n",
    "\n",
    "        typical_error = typical_error/500\n",
    "        return typical_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For k = 4 and m = 30, generate data and fit a decision tree to it. Does the ordering of the variables in the decision tree make sense, based on the function that defines Y ? Why or why not? Draw the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': {0: {'a': {0: 1, 1: {'b': {0: 0, 1: 1}}}},\n",
      "       1: {'a': {0: 0, 1: {'b': {0: 0, 1: 1}}}}}}\n"
     ]
    }
   ],
   "source": [
    "tree = dt.build_tree(train_data)\n",
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = dt.fit(train_data, tree)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Write a function that takes a decision tree and estimates its typical error on this data $err(\\hat{f})$; i.e., generate a lot of data according to the above scheme, and find the average error rate of this tree over that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 101.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1246000000000002"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typical_error = dt.generate_data_and_typical_error(tree, k, m)\n",
    "typical_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. For k = 10, estimate the value of $|err_{train}(\\hat{f}) − err(\\hat{f})|$ for a given m by repeatedly generating data sets, fitting trees to those data sets, and estimating the true and training error. Do this for multiple m, and graph this difference as a function of m. What can you say about the marginal value of additional training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_varied_m():\n",
    "    k = 10\n",
    "    m = list(range(10, 50))\n",
    "    errors = []\n",
    "    for sample_size in m:\n",
    "        train_data = create_dataset(k, sample_size)\n",
    "        dt = DecisionTree()\n",
    "        tree = dt.build_tree(train_data)\n",
    "        train_error = dt.fit(train_data, tree)\n",
    "        typical_error = generate_data_and_typical_error(tree, k, sample_size)\n",
    "        errors.append(abs(train_error - typical_error))\n",
    "    plt.plot(m, errors)\n",
    "    plt.xlabel(\"Value of m (sample size)\")\n",
    "    plt.ylabel(\"Abs. difference between training and true error\")\n",
    "    plt.title(\"Error difference as a function of m\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 170.93it/s]\n",
      "100%|██████████| 500/500 [00:03<00:00, 155.40it/s]\n",
      "100%|██████████| 500/500 [00:03<00:00, 138.20it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 113.63it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 109.41it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 105.80it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 106.99it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 103.99it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 90.76it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 77.76it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 82.19it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 80.83it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 74.32it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 75.72it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 72.88it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 68.89it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 67.50it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 67.70it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 61.13it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 54.76it/s]\n",
      "100%|██████████| 500/500 [00:08<00:00, 56.43it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 54.80it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.91it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 53.62it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.52it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.12it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.58it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 38.04it/s]\n",
      "100%|██████████| 500/500 [00:11<00:00, 42.26it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 41.48it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 42.37it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 40.30it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 37.76it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 36.92it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 35.27it/s]\n",
      "100%|██████████| 500/500 [00:14<00:00, 35.53it/s]\n",
      "100%|██████████| 500/500 [00:14<00:00, 34.12it/s]\n",
      "100%|██████████| 500/500 [00:14<00:00, 33.54it/s]\n",
      "100%|██████████| 500/500 [00:15<00:00, 31.92it/s]\n",
      " 66%|██████▋   | 332/500 [00:10<00:05, 31.71it/s]"
     ]
    }
   ],
   "source": [
    "generate_data_varied_m()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Design an alternative metric for splitting the data, not based on information content / information gain. Repeat the computation from (5) above for your metric, and compare the performance of your trees vs the ID3 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
