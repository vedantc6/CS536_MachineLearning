{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS536: Perceptrons\n",
    "#### Done by - Vedant Choudhary, vc389\n",
    "In the usual way, we need data that we can fit and analyze using perceptrons. Consider generating data points (X, Y) in the following way:\n",
    "- For $i = 1,....,k-1$, let $X_i ~ N(0, 1)$ (i.e. each $X_i$ is an i.i.d. standard normal)\n",
    "- For $i = k$, generate $X_k$ in the following way: let $D ~ Exp(1)$, and for a parameter $\\epsilon > 0$ take\n",
    "\n",
    "$X_k = (\\epsilon + D)$ with probability 1/2\n",
    "\n",
    "$X_k = -(\\epsilon + D)$ with probability 1/2\n",
    "\n",
    "The effect of this is that while $X_1,...X_{k-1}$ are i.i.d. standard normals, $X_k$ is distributed randomly with some gap (of size $2\\epsilon$ around $X_k = 0$. We can then classify each point according to the following:\n",
    "\n",
    "$Y = 1$ with probability 1/2\n",
    "\n",
    "$Y = -1$ with probability 1/2\n",
    "\n",
    "We see that the class of each data point is determined entirely by the value of the $X_k$ feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Show that there is a perceptron that correctly classifies this data. Is this perceptron unique? What is the ‘best’ perceptron for this data set, theoretically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X (feature) vectors for the data\n",
    "def create_data(k, m, D, epsilon):\n",
    "    X_k_minus_1 = np.random.normal(0, 1, (m,k-1))\n",
    "    X_k = []\n",
    "    for i in range(m):\n",
    "        temp = np.random.choice(2, 1, p=[0.5,0.5])\n",
    "#         print(temp)\n",
    "        if temp == 1:\n",
    "            X_k.append(epsilon + D)\n",
    "        else:\n",
    "            X_k.append(-(epsilon + D))\n",
    "    X_k = np.asarray(X_k).reshape((1,m))\n",
    "#     print(X_k_minus_1)\n",
    "#     print(X_k)\n",
    "    return np.concatenate((X_k_minus_1, X_k.T), axis=1)\n",
    "\n",
    "# Creating target column for the data\n",
    "def create_y(X, m):\n",
    "    y = []\n",
    "    for i in range(m):\n",
    "        if X[i][-1] > 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(-1)\n",
    "    return y\n",
    "\n",
    "# Combining all the sub data points into a dataframe\n",
    "def create_dataset(k, m, epsilon, D):\n",
    "    X = np.asarray(create_data(k, m, epsilon, D))\n",
    "    y = np.asarray(create_y(X, m)).reshape((m,1))\n",
    "\n",
    "    # Training data is an appended version of X and y arrays\n",
    "    data = pd.DataFrame(np.append(X, y, axis=1), columns=[\"X\" + str(i) for i in range(1,k+1)]+['Y'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.052815</td>\n",
       "      <td>1.041992</td>\n",
       "      <td>-0.317840</td>\n",
       "      <td>0.461445</td>\n",
       "      <td>-1.219667</td>\n",
       "      <td>1.736444</td>\n",
       "      <td>0.074190</td>\n",
       "      <td>0.521638</td>\n",
       "      <td>-0.096941</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929177</td>\n",
       "      <td>-0.244840</td>\n",
       "      <td>-1.448114</td>\n",
       "      <td>-1.042703</td>\n",
       "      <td>1.376720</td>\n",
       "      <td>0.850087</td>\n",
       "      <td>-0.245150</td>\n",
       "      <td>0.356574</td>\n",
       "      <td>-4.080852</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506920</td>\n",
       "      <td>0.383559</td>\n",
       "      <td>-1.389646</td>\n",
       "      <td>-0.545733</td>\n",
       "      <td>-0.189458</td>\n",
       "      <td>0.867507</td>\n",
       "      <td>0.378216</td>\n",
       "      <td>-0.898799</td>\n",
       "      <td>0.644921</td>\n",
       "      <td>0.929950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044940</td>\n",
       "      <td>0.388951</td>\n",
       "      <td>0.300723</td>\n",
       "      <td>0.922243</td>\n",
       "      <td>-1.963112</td>\n",
       "      <td>0.987887</td>\n",
       "      <td>0.339391</td>\n",
       "      <td>-1.914598</td>\n",
       "      <td>4.080852</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.231154</td>\n",
       "      <td>1.529727</td>\n",
       "      <td>-0.058887</td>\n",
       "      <td>-1.401355</td>\n",
       "      <td>-1.128395</td>\n",
       "      <td>-1.071578</td>\n",
       "      <td>-1.037940</td>\n",
       "      <td>-0.101086</td>\n",
       "      <td>-0.632556</td>\n",
       "      <td>1.178592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.061076</td>\n",
       "      <td>-1.232727</td>\n",
       "      <td>1.315680</td>\n",
       "      <td>0.749097</td>\n",
       "      <td>-0.641943</td>\n",
       "      <td>-0.099577</td>\n",
       "      <td>0.955176</td>\n",
       "      <td>-1.204766</td>\n",
       "      <td>4.080852</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.924930</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.743430</td>\n",
       "      <td>-0.766712</td>\n",
       "      <td>-0.442100</td>\n",
       "      <td>-0.738759</td>\n",
       "      <td>-0.880178</td>\n",
       "      <td>1.090554</td>\n",
       "      <td>0.495343</td>\n",
       "      <td>-0.161814</td>\n",
       "      <td>...</td>\n",
       "      <td>1.370903</td>\n",
       "      <td>-0.796894</td>\n",
       "      <td>0.794242</td>\n",
       "      <td>-0.729538</td>\n",
       "      <td>-1.877562</td>\n",
       "      <td>1.651299</td>\n",
       "      <td>-0.115813</td>\n",
       "      <td>1.759965</td>\n",
       "      <td>-4.080852</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.238222</td>\n",
       "      <td>0.058716</td>\n",
       "      <td>-0.602490</td>\n",
       "      <td>-0.659524</td>\n",
       "      <td>0.759971</td>\n",
       "      <td>1.450825</td>\n",
       "      <td>-0.602260</td>\n",
       "      <td>-0.263689</td>\n",
       "      <td>1.305453</td>\n",
       "      <td>-0.855617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150832</td>\n",
       "      <td>2.051842</td>\n",
       "      <td>1.531062</td>\n",
       "      <td>0.705523</td>\n",
       "      <td>1.872988</td>\n",
       "      <td>1.488366</td>\n",
       "      <td>-1.377275</td>\n",
       "      <td>-0.577903</td>\n",
       "      <td>-4.080852</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0 -0.052815  1.041992 -0.317840  0.461445 -1.219667  1.736444  0.074190   \n",
       "1  0.506920  0.383559 -1.389646 -0.545733 -0.189458  0.867507  0.378216   \n",
       "2 -1.231154  1.529727 -0.058887 -1.401355 -1.128395 -1.071578 -1.037940   \n",
       "3 -0.924930  0.245410  0.743430 -0.766712 -0.442100 -0.738759 -0.880178   \n",
       "4  0.238222  0.058716 -0.602490 -0.659524  0.759971  1.450825 -0.602260   \n",
       "\n",
       "         X8        X9       X10 ...        X12       X13       X14       X15  \\\n",
       "0  0.521638 -0.096941  0.018287 ...  -0.929177 -0.244840 -1.448114 -1.042703   \n",
       "1 -0.898799  0.644921  0.929950 ...   0.044940  0.388951  0.300723  0.922243   \n",
       "2 -0.101086 -0.632556  1.178592 ...  -1.061076 -1.232727  1.315680  0.749097   \n",
       "3  1.090554  0.495343 -0.161814 ...   1.370903 -0.796894  0.794242 -0.729538   \n",
       "4 -0.263689  1.305453 -0.855617 ...  -0.150832  2.051842  1.531062  0.705523   \n",
       "\n",
       "        X16       X17       X18       X19       X20    Y  \n",
       "0  1.376720  0.850087 -0.245150  0.356574 -4.080852 -1.0  \n",
       "1 -1.963112  0.987887  0.339391 -1.914598  4.080852  1.0  \n",
       "2 -0.641943 -0.099577  0.955176 -1.204766  4.080852  1.0  \n",
       "3 -1.877562  1.651299 -0.115813  1.759965 -4.080852 -1.0  \n",
       "4  1.872988  1.488366 -1.377275 -0.577903 -4.080852 -1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables - k = 20, m = 100, epsilon = 1\n",
    "k, m, epsilon = 20, 100, 1\n",
    "D = float(np.random.exponential(1, 1))\n",
    "\n",
    "train_data = create_dataset(k, m, epsilon, D)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Perceptron\n",
    "class Perceptron():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sign_function(self, data_vec):\n",
    "        return np.array([1 if val >= 1 else -1 for val in data_vec])[:, np.newaxis]\n",
    "    \n",
    "    def pla(self, data):\n",
    "        X = np.asarray(data.iloc[:,:-1])\n",
    "        y = np.asarray(data.iloc[:,-1:])\n",
    "        num_samples, num_features = X.shape\n",
    "#         Initialize weight and bias parameters\n",
    "        self.w = np.zeros(shape=(num_features, 1))\n",
    "        self.bias = 0\n",
    "#         Add check condition if y != f_x ever\n",
    "        while True:\n",
    "#             Calculate the mapping function f(x)\n",
    "            f_x = self.sign_function(np.dot(X, self.w) + self.bias)\n",
    "#             Compute weights if f_x != y\n",
    "            for i in range(num_samples):\n",
    "                if f_x[i] != y[i]:\n",
    "                    self.w += np.dot(X[i].T.reshape((num_features, 1)), y[i].reshape((1,1)))\n",
    "                    self.bias += y[i]\n",
    "                \n",
    "            if np.array_equal(y, f_x):\n",
    "                break\n",
    "                    \n",
    "        return self.w, self.bias\n",
    "    \n",
    "    '''\n",
    "    Predicts the target value based on a data vector\n",
    "    Input - a single row of dataset or a single X vector, decision tree\n",
    "    Return - predicted value\n",
    "    '''\n",
    "    def predict(self, instance_data):\n",
    "        prediction = self.sign_function(np.dot(self.w.T, np.asarray(instance_data).reshape((20,1))) + self.bias)\n",
    "        return prediction   \n",
    "    \n",
    "    '''\n",
    "    Predicts the target value and then calculates error based on the predictions\n",
    "    Input - dataset, decision tree built\n",
    "    Return - error\n",
    "    '''\n",
    "    def fit(self, data):\n",
    "        error = 0\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data.iloc[i][:-1])\n",
    "            if prediction != data.iloc[i][-1]:\n",
    "                error += 1\n",
    "        return error/len(data) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.25373354]\n",
      " [ -3.45473026]\n",
      " [  4.14668222]\n",
      " [-10.81635368]\n",
      " [ -5.58331826]\n",
      " [  0.79608837]\n",
      " [ -5.93601992]\n",
      " [ -1.63990473]\n",
      " [  3.63995818]\n",
      " [ -9.70885884]\n",
      " [  4.72328779]\n",
      " [ 13.90303297]\n",
      " [  7.08709852]\n",
      " [ -4.69182138]\n",
      " [ -3.33967112]\n",
      " [ -8.14006658]\n",
      " [  9.43349819]\n",
      " [ 11.43870612]\n",
      " [ -3.53964662]\n",
      " [179.55747274]]\n",
      "[44.]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "final_w, final_b = perceptron.pla(train_data)\n",
    "print(final_w)\n",
    "print(final_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = perceptron.fit(train_data)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.25373354],\n",
       "       [ -3.45473026],\n",
       "       [  4.14668222],\n",
       "       [-10.81635368],\n",
       "       [ -5.58331826],\n",
       "       [  0.79608837],\n",
       "       [ -5.93601992],\n",
       "       [ -1.63990473],\n",
       "       [  3.63995818],\n",
       "       [ -9.70885884],\n",
       "       [  4.72328779],\n",
       "       [ 13.90303297],\n",
       "       [  7.08709852],\n",
       "       [ -4.69182138],\n",
       "       [ -3.33967112],\n",
       "       [ -8.14006658],\n",
       "       [  9.43349819],\n",
       "       [ 11.43870612],\n",
       "       [ -3.53964662],\n",
       "       [179.55747274]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
